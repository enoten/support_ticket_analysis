{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc1bdc46-52bd-47e9-a034-9cc21846faac",
   "metadata": {},
   "source": [
    "<img src=\"fine_tune_LLMs.jpg\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367c7131-c3c1-414a-89f1-a337e2508aac",
   "metadata": {},
   "source": [
    "HREF to Video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6285a9-3535-4b42-bddc-28e2ae7a26a4",
   "metadata": {},
   "source": [
    "# -1 Research Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e405274-45b0-45b0-80a8-a59ad93aafaf",
   "metadata": {},
   "source": [
    "<fieldset>\n",
    "<legend>References</legend>\n",
    "1. Hinton, G. E., Osindero, S., & Teh, Y. W. (2006). <a href=\"https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf\">A fast learning algorithm for deep belief nets.</a> <i>Neural computation</i>, 18(7), p. 1527-1554. <br>\n",
    "2. Larochelle, H., Bengio, Y., Louradour, J., & Lamblin, P. (2009). <a href=\"https://www.jmlr.org/papers/volume10/larochelle09a/larochelle09a.pdf\">Exploring strategies for training deep neural networks.</a> <i>Journal of machine learning research</i>, 10(1), p. 1-40.\n",
    "</fieldset>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a623e1-006e-469a-8497-7911afcbe2fc",
   "metadata": {},
   "source": [
    "Snapshots from Larochelle et al. (2009) Exploring strategies for training deep neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7b4059-934f-4aa2-b3e6-5ea1c4033343",
   "metadata": {},
   "source": [
    "<table style=\"border:1; border-color:red\">\n",
    "<th>\n",
    "<td width=60% align='left'><img src=\"yoshio_bengio_2009_snapshot_1.jpg\"></td>\n",
    "<td align='left'><img src=\"yoshio_bengio_2009_snapshot_2.jpg\" width=110%></td>\n",
    "</th>    \n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7a0912-dfcb-44cd-9e89-4099d0b36855",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e9a8f8b-4e90-4a73-b696-25c9a430a0db",
   "metadata": {},
   "source": [
    "# 0. Training Multi-Layer Perceptrons (MLPs): Pre-training and Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6732485-584c-47d1-9262-48b9dd328d14",
   "metadata": {},
   "source": [
    "<b>0.1 MLP Structure</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b06372c-d264-4ba5-9fd1-d2d04e75c3da",
   "metadata": {},
   "source": [
    "<img src=\"mlp.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161ee59e-c06c-4385-acc9-9579cc27d995",
   "metadata": {},
   "source": [
    "<b>0.2 MLP Functionals Components</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b598125-5212-4715-9736-3af0bda35ca3",
   "metadata": {},
   "source": [
    "<img src=\"mlp_functional_schema.jpg\" width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b89ee9-62ad-4bfa-9cc4-097aa241b86b",
   "metadata": {},
   "source": [
    "<b>0.3 Restricted Boltzmann Machine (RMB)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f53f375-ce4e-478e-bbb7-3820bfa12d34",
   "metadata": {},
   "source": [
    "<img src=\"rbm.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50678b5d-2bef-4868-8038-b549c911f881",
   "metadata": {},
   "source": [
    "<b>0.4 Stacked RMBs</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480539d4-3c9d-4e9c-a368-e0c2a58aabff",
   "metadata": {},
   "source": [
    "<img src=\"stacked_rbms.jpg\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4cd0e0-ba01-4fa9-8b0d-d2752a9f82fb",
   "metadata": {},
   "source": [
    "<b>0.5 Pre-train MLP with RBMs</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ea9ac8-9fdf-45b2-ab3d-5a2b06ba995b",
   "metadata": {},
   "source": [
    "<img src=\"pre_train_MLP_with_RBMs.jpg\" width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6de74b-c72b-496f-946c-a0cfb751c936",
   "metadata": {},
   "source": [
    "<b>0.6 Fine-Tuning Pre-Trained MLP</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829e7305-5166-45af-9723-84b65929d932",
   "metadata": {},
   "source": [
    "<img src=\"fine_tuning_MLP.jpg\" width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483e33fc-b5b8-487c-bf36-04c399b73281",
   "metadata": {},
   "source": [
    "# 1. Transfer Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cee7be6-d87d-46e0-bc16-6e3d623b6981",
   "metadata": {},
   "source": [
    "<b>1.1 Transfer Learning: Basic Concept</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2a7045-c33b-4a37-921d-1139bf512f31",
   "metadata": {},
   "source": [
    "<img src=\"transfer_learning.jpg\" width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6920a9cd-34cc-4a0d-a67d-1ad16583001d",
   "metadata": {},
   "source": [
    "<b>1.2 Convolutional Neural Networks: VGG16</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6867f9c-8239-49e6-8642-c21b68f9364c",
   "metadata": {},
   "source": [
    "<img src=\"vgg16_architecture.jpg\" width=60%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2dfb5c-367d-48ce-8947-69bd21afca85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c51ffc67-18e1-43ef-8e63-99a80c886c64",
   "metadata": {},
   "source": [
    "<img src=\"vgg16_functional_schema.jpg\" width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b2031e-5824-4827-aa34-1394d3485e55",
   "metadata": {},
   "source": [
    "<b>1.2 Transfer Learning Quick Examples with VGG16</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a8fdf1-2adb-4f04-810e-613047143ccb",
   "metadata": {},
   "source": [
    "<img src=\"transfer_learning_for_CNNs.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8a09be7-9c2a-412b-82d0-848b7819dfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load EuroSAT dataset\n",
    "import numpy as np\n",
    "\n",
    "euroSAT_load = np.load('euroSAT_dataset.npy',allow_pickle=True)\n",
    "\n",
    "euroSAT_dataset = euroSAT_load[0].copy()\n",
    "\n",
    "euroSAT_dataset.keys()\n",
    "\n",
    "num_samples = len(euroSAT_dataset['labels'])\n",
    "\n",
    "dataset_index = np.random.permutation(num_samples)\n",
    "\n",
    "train_ratio = 0.7\n",
    "valid_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "train_cut = int(num_samples*train_ratio)\n",
    "valid_cut = int(num_samples*(train_ratio+valid_ratio))\n",
    "\n",
    "train_ind = dataset_index[:train_cut]\n",
    "valid_ind = dataset_index[train_cut:valid_cut]\n",
    "test_ind = dataset_index[valid_cut:]\n",
    "\n",
    "len(train_ind) + len(valid_ind) + len(test_ind)\n",
    "\n",
    "train_ind = np.squeeze(np.array(train_ind))\n",
    "valid_ind = np.squeeze(np.array(valid_ind))\n",
    "test_ind = np.squeeze(np.array(test_ind))\n",
    "\n",
    "train_img = euroSAT_dataset['images'][train_ind]\n",
    "train_img = train_img/255\n",
    "\n",
    "valid_img = euroSAT_dataset['images'][valid_ind]\n",
    "valid_img = valid_img/255\n",
    "\n",
    "test_img = euroSAT_dataset['images'][test_ind]\n",
    "test_img = test_img/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "195591e8-0cc7-4b0c-b1d2-c4f60757fa5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18900, 64, 64, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ab6c07c-8328-4653-abbb-ab828bd615fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eurosat_img_shape = train_img.shape[1:]\n",
    "eurosat_img_shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fba6452-53c2-4015-ac58-cd66a84a5e7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3d2e13b-2eea-4f18-a497-218b6327be4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "834ad650-c9d6-4158-8716-c0c8132ca697",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e05f20c-455f-418b-8a06-fc1c87dd50c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.array(euroSAT_dataset['labels'])[train_ind]\n",
    "train_labels = keras.utils.to_categorical(train_labels)\n",
    "\n",
    "valid_labels = np.array(euroSAT_dataset['labels'])[valid_ind]\n",
    "valid_labels = keras.utils.to_categorical(valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b961e8bf-4364-43cc-a9e6-4ac6acedc8b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18e9349-7fde-47ce-a750-78be84112c86",
   "metadata": {},
   "source": [
    "Step 1. Load trained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b53ce58-baf2-4db8-97ba-3ca7159b2d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_model = keras.applications.vgg16.VGG16(weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "20d88bf2-6994-408c-934f-eacc1dcf023f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      " predictions (Dense)         (None, 1000)              4097000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 138357544 (527.79 MB)\n",
      "Trainable params: 138357544 (527.79 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg16_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f67a55bc-51c1-47b9-849b-b4e475b02c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = keras.applications.vgg16.VGG16(weights='imagenet',\n",
    "                                            include_top=False,\n",
    "                                            input_shape=eurosat_img_shape\n",
    "                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d1ddd373-d6e6-4de5-9820-5d2a60b629fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 64, 64, 3)]       0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 64, 64, 64)        1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 64, 64, 64)        36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 32, 32, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 32, 32, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 16, 16, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 16, 16, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 16, 16, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 8, 8, 512)         1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 4, 4, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14714688 (56.13 MB)\n",
      "Trainable params: 14714688 (56.13 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6dbee171-e2f1-40b7-8ca7-190660b7c9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base = keras.applications.vgg16.VGG16(weights='imagenet',\n",
    "                                           include_top=False,\n",
    "                                           input_shape=eurosat_img_shape)\n",
    "\n",
    "transfer_layer = conv_base.get_layer('block5_pool')\n",
    "conv_base.trainable = False\n",
    "\n",
    "x = keras.layers.Flatten()(transfer_layer.output)\n",
    "x = keras.layers.Dense(256)(x)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "pred = keras.layers.Dense(len(euroSAT_dataset['classes']), activation='softmax')(x)\n",
    "\n",
    "tl_model = keras.Model(conv_base.input,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cae9a62c-0bc1-463e-9737-9b1b1af37964",
   "metadata": {},
   "outputs": [],
   "source": [
    "tl_model.compile(loss=\"categorical_crossentropy\",\n",
    "                 optimizer=\"rmsprop\",\n",
    "                 metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "28dddcc7-4eb7-4b91-9377-25b747156c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 64, 64, 3)]       0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 64, 64, 64)        1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 64, 64, 64)        36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 32, 32, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 32, 32, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 16, 16, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 16, 16, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 16, 16, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 8, 8, 512)         1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 4, 4, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               524544    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15241802 (58.14 MB)\n",
      "Trainable params: 527114 (2.01 MB)\n",
      "Non-trainable params: 14714688 (56.13 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#tl_model.build((180,180,3))\n",
    "tl_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ec5e67a0-b2ac-414e-ba81-285cb9f43c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e97938d5-923e-4ecb-a9f5-59f0510e7c43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.13.0'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e03996b9-3546-4503-a646-98ba0e0545b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1182/1182 [==============================] - 213s 180ms/step - loss: 0.9408 - accuracy: 0.7129 - val_loss: 0.5246 - val_accuracy: 0.8227\n",
      "Epoch 2/20\n",
      "1182/1182 [==============================] - 224s 190ms/step - loss: 0.6537 - accuracy: 0.7999 - val_loss: 0.6944 - val_accuracy: 0.7773\n",
      "Epoch 3/20\n",
      "1182/1182 [==============================] - 218s 185ms/step - loss: 0.5907 - accuracy: 0.8170 - val_loss: 0.5171 - val_accuracy: 0.8353\n",
      "Epoch 4/20\n",
      "1182/1182 [==============================] - 220s 186ms/step - loss: 0.5407 - accuracy: 0.8321 - val_loss: 0.5308 - val_accuracy: 0.8326\n",
      "Epoch 5/20\n",
      "1182/1182 [==============================] - 221s 187ms/step - loss: 0.5182 - accuracy: 0.8384 - val_loss: 0.6146 - val_accuracy: 0.8151\n",
      "Epoch 6/20\n",
      "1182/1182 [==============================] - 218s 185ms/step - loss: 0.4999 - accuracy: 0.8500 - val_loss: 0.4939 - val_accuracy: 0.8479\n",
      "Epoch 7/20\n",
      "1182/1182 [==============================] - 216s 183ms/step - loss: 0.4821 - accuracy: 0.8515 - val_loss: 0.4691 - val_accuracy: 0.8533\n",
      "Epoch 8/20\n",
      "1182/1182 [==============================] - 215s 182ms/step - loss: 0.4689 - accuracy: 0.8534 - val_loss: 0.4452 - val_accuracy: 0.8667\n",
      "Epoch 9/20\n",
      "1182/1182 [==============================] - 212s 179ms/step - loss: 0.4566 - accuracy: 0.8562 - val_loss: 0.5423 - val_accuracy: 0.8398\n",
      "Epoch 10/20\n",
      "1182/1182 [==============================] - 214s 181ms/step - loss: 0.4576 - accuracy: 0.8597 - val_loss: 0.4780 - val_accuracy: 0.8514\n",
      "Epoch 11/20\n",
      "1182/1182 [==============================] - 217s 183ms/step - loss: 0.4423 - accuracy: 0.8656 - val_loss: 0.5642 - val_accuracy: 0.8348\n",
      "Epoch 12/20\n",
      "1182/1182 [==============================] - 219s 185ms/step - loss: 0.4362 - accuracy: 0.8659 - val_loss: 0.4902 - val_accuracy: 0.8511\n",
      "Epoch 13/20\n",
      "1182/1182 [==============================] - 216s 183ms/step - loss: 0.4284 - accuracy: 0.8688 - val_loss: 0.4540 - val_accuracy: 0.8657\n",
      "Epoch 14/20\n",
      "1182/1182 [==============================] - 217s 184ms/step - loss: 0.4227 - accuracy: 0.8695 - val_loss: 0.5364 - val_accuracy: 0.8484\n",
      "Epoch 15/20\n",
      "1182/1182 [==============================] - 217s 183ms/step - loss: 0.4195 - accuracy: 0.8737 - val_loss: 0.5249 - val_accuracy: 0.8486\n",
      "Epoch 16/20\n",
      "1182/1182 [==============================] - 217s 184ms/step - loss: 0.4097 - accuracy: 0.8763 - val_loss: 0.6265 - val_accuracy: 0.8249\n",
      "Epoch 17/20\n",
      "1182/1182 [==============================] - 216s 183ms/step - loss: 0.4031 - accuracy: 0.8736 - val_loss: 0.4493 - val_accuracy: 0.8684\n",
      "Epoch 18/20\n",
      "1182/1182 [==============================] - 217s 183ms/step - loss: 0.4035 - accuracy: 0.8775 - val_loss: 0.4806 - val_accuracy: 0.8578\n",
      "Epoch 19/20\n",
      "1182/1182 [==============================] - 217s 184ms/step - loss: 0.4026 - accuracy: 0.8770 - val_loss: 0.5385 - val_accuracy: 0.8477\n",
      "Epoch 20/20\n",
      "1182/1182 [==============================] - 218s 184ms/step - loss: 0.4004 - accuracy: 0.8802 - val_loss: 0.5356 - val_accuracy: 0.8491\n"
     ]
    }
   ],
   "source": [
    "history = tl_model.fit(train_img,\n",
    "                       train_labels,\n",
    "                       batch_size=16,\n",
    "                       epochs=20,\n",
    "#                       steps_per_epoch=100,\n",
    "#                       #verbose=\"auto\",\n",
    "#                       #callbacks=None,\n",
    "                       validation_data=(valid_img, valid_labels),\n",
    "#                       validation_steps=100\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6aba82c2-a64a-4ff5-96a0-f74eeb3a414b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1182/1182 [==============================] - 232s 196ms/step - loss: 0.3940 - accuracy: 0.8845 - val_loss: 0.4898 - val_accuracy: 0.8627\n",
      "Epoch 2/20\n",
      "1182/1182 [==============================] - 219s 185ms/step - loss: 0.3921 - accuracy: 0.8820 - val_loss: 0.4926 - val_accuracy: 0.8640\n",
      "Epoch 3/20\n",
      "1182/1182 [==============================] - 216s 183ms/step - loss: 0.3875 - accuracy: 0.8844 - val_loss: 0.4657 - val_accuracy: 0.8723\n",
      "Epoch 4/20\n",
      "1182/1182 [==============================] - 217s 184ms/step - loss: 0.3873 - accuracy: 0.8812 - val_loss: 0.4904 - val_accuracy: 0.8625\n",
      "Epoch 5/20\n",
      "1182/1182 [==============================] - 216s 183ms/step - loss: 0.3817 - accuracy: 0.8830 - val_loss: 0.5841 - val_accuracy: 0.8506\n",
      "Epoch 6/20\n",
      "1182/1182 [==============================] - 216s 183ms/step - loss: 0.3824 - accuracy: 0.8861 - val_loss: 0.4686 - val_accuracy: 0.8716\n",
      "Epoch 7/20\n",
      "1182/1182 [==============================] - 217s 184ms/step - loss: 0.3739 - accuracy: 0.8859 - val_loss: 0.5256 - val_accuracy: 0.8568\n",
      "Epoch 8/20\n",
      "1182/1182 [==============================] - 217s 183ms/step - loss: 0.3693 - accuracy: 0.8911 - val_loss: 0.4871 - val_accuracy: 0.8672\n",
      "Epoch 9/20\n",
      "1182/1182 [==============================] - 223s 189ms/step - loss: 0.3639 - accuracy: 0.8902 - val_loss: 0.4961 - val_accuracy: 0.8659\n",
      "Epoch 10/20\n",
      "1182/1182 [==============================] - 240s 203ms/step - loss: 0.3670 - accuracy: 0.8891 - val_loss: 0.4596 - val_accuracy: 0.8815\n",
      "Epoch 11/20\n",
      "1182/1182 [==============================] - 230s 194ms/step - loss: 0.3728 - accuracy: 0.8910 - val_loss: 0.4505 - val_accuracy: 0.8805\n",
      "Epoch 12/20\n",
      "1182/1182 [==============================] - 248s 210ms/step - loss: 0.3623 - accuracy: 0.8922 - val_loss: 0.6654 - val_accuracy: 0.8284\n",
      "Epoch 13/20\n",
      "1182/1182 [==============================] - 248s 210ms/step - loss: 0.3643 - accuracy: 0.8928 - val_loss: 0.5531 - val_accuracy: 0.8573\n",
      "Epoch 14/20\n",
      "1182/1182 [==============================] - 225s 190ms/step - loss: 0.3645 - accuracy: 0.8899 - val_loss: 0.5723 - val_accuracy: 0.8474\n",
      "Epoch 15/20\n",
      "1182/1182 [==============================] - 219s 185ms/step - loss: 0.3481 - accuracy: 0.8929 - val_loss: 0.5759 - val_accuracy: 0.8467\n",
      "Epoch 16/20\n",
      "1182/1182 [==============================] - 238s 201ms/step - loss: 0.3533 - accuracy: 0.8946 - val_loss: 0.5448 - val_accuracy: 0.8602\n",
      "Epoch 17/20\n",
      "1182/1182 [==============================] - 253s 214ms/step - loss: 0.3508 - accuracy: 0.8959 - val_loss: 0.4949 - val_accuracy: 0.8738\n",
      "Epoch 18/20\n",
      "1182/1182 [==============================] - 244s 207ms/step - loss: 0.3582 - accuracy: 0.8954 - val_loss: 0.5138 - val_accuracy: 0.8691\n",
      "Epoch 19/20\n",
      "1182/1182 [==============================] - 247s 209ms/step - loss: 0.3501 - accuracy: 0.8954 - val_loss: 0.4913 - val_accuracy: 0.8699\n",
      "Epoch 20/20\n",
      "1182/1182 [==============================] - 251s 213ms/step - loss: 0.3461 - accuracy: 0.8980 - val_loss: 0.5376 - val_accuracy: 0.8704\n"
     ]
    }
   ],
   "source": [
    "history1 = tl_model.fit(train_img,\n",
    "                       train_labels,\n",
    "                       batch_size=16,\n",
    "                       epochs=20,\n",
    "#                       steps_per_epoch=100,\n",
    "#                       #verbose=\"auto\",\n",
    "#                       #callbacks=None,\n",
    "                       validation_data=(valid_img, valid_labels),\n",
    "#                       validation_steps=100\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b0c2a3-1870-464c-adb9-6af1cc9b4f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2d702d-9efc-4060-8383-4a8d8b6a2297",
   "metadata": {},
   "source": [
    "## 0. TRANSFORMER vs. BERT Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192b2cc9-6703-4c96-b82e-a9efb98c4c64",
   "metadata": {},
   "source": [
    "<img src=\"transformers.jpg\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af961c22-0da7-4cb2-b950-29fed0e234df",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_imgnet = load_dataset(\"zh-plus/tiny-imagenet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527016fb-5a36-45b5-af9c-a1c1e192a5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(mini_imgnet['valid']['label'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23de9f46-a607-4875-be36-029b6935abeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(mini_imgnet['valid']['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c004a7db-ba5c-43db-a0d2-50763894a2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_array = np.array([])\n",
    "valid_imgs = mini_imgnet['valid']['image'].copy()\n",
    "for i, img in enumerate(valid_imgs):\n",
    "    new_image = img.resize((200, 200))\n",
    "    valid_array = np.stack([valid_array,np.array(new_image)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5260c0f1-7f38-4753-bef3-c787925c46b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_array = np.array(valid_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcecb19f-e039-451f-ac26-95068a594f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs = mini_imgnet['train']['image'].copy()\n",
    "for i, img in enumerate(train_imgs):\n",
    "    train_imgs[i] = np.array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eedd0a3-35ac-4eeb-a78d-26a166e97966",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = keras.utils.to_categorical(mini_imgnet['train']['label'],\n",
    "                                         num_classes=200)\n",
    "valid_labels = keras.utils.to_categorical(mini_imgnet['valid']['label'],\n",
    "                                         num_classes=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df66b21-494e-48b9-9e97-9cfb0fba21be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988120da-9a3b-464c-bc42-c3f7823eefe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77f17474-1423-418d-b369-3743abba6b40",
   "metadata": {},
   "source": [
    "### Fine-tune BERT Base model for Support Ticket Dataset Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32b5c69-b8b2-4807-b6c5-d5ed3d237b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import (logging,\n",
    "    AutoTokenizer, DataCollatorWithPadding, \n",
    "    AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "    )\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ignore warnings from transformers lib\n",
    "logging.set_verbosity_error() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c799a68-36aa-4ecc-8c57-2becf92b38e2",
   "metadata": {},
   "source": [
    "### 1. Load data from Support Tickets Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b3f3f9-0e19-4583-808a-209e22651fdf",
   "metadata": {},
   "source": [
    "<b>1.1 Load Support Tickets Dataset fom HuggingFace Hub</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fbd278-3ace-49cb-9765-5dbed896d5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "support_tickets = load_dataset(\"phi-ai-info/support_tickets\",name='alpha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb6b85e-9b43-44cc-a335-e415635d7c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "support_tickets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ed3c2d-9e57-4bd0-b880-5044760613be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_to_df(dataset_obj, \n",
    "                  label=None,\n",
    "                  reference='class',\n",
    "                  new_key='label'\n",
    "                 ):\n",
    "    cols = list(dataset_obj[0].keys())\n",
    "\n",
    "    cols_values = {item:[] for item in cols}\n",
    "    for json_rec in dataset_obj:\n",
    "        for item in cols:\n",
    "            cols_values[item].append(json_rec[item])\n",
    "    \n",
    "    if label is not None:\n",
    "        cols += [new_key]\n",
    "        cols_values[new_key] = []\n",
    "        for json_rec in dataset_obj:\n",
    "            cols_values[new_key].append(label[json_rec[reference]])\n",
    "    \n",
    "    return pd.DataFrame({item:cols_values[item] for item in cols})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaabd4f-d473-41bb-9115-230c68930e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_class_dict = {'revoke access': 'access',\n",
    " 'grant access': 'access',\n",
    " 'access profile': 'access',\n",
    " 'add user': 'user',\n",
    " 'delete user': 'user',\n",
    " 'create user': 'user',\n",
    " 'modify user': 'user',\n",
    " 'user role': 'user',\n",
    " 'disk space': 'storage',\n",
    " 'hard disk': 'storage',\n",
    " 'disk full': 'storage',\n",
    " 'ssd disk': 'storage',\n",
    " 'disk error': 'storage',\n",
    " 'shared disk': 'storage',\n",
    " 'nas disk': 'storage',\n",
    " 'printer functioning': 'printer',\n",
    " 'printer driver': 'printer',\n",
    " 'printer toner': 'printer',\n",
    " 'printer paper': 'printer',\n",
    " 'wifi functioning': 'network',\n",
    " 'network functioning': 'network',\n",
    " 'email server': 'servers',\n",
    " 'web server': 'servers'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc4500b-5200-4278-8b31-3e7c27dd4c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_name_dict = {item: i for i,item in enumerate(list(set(label_class_dict.values())))}\n",
    "label_name_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973fbef4-cecb-4a47-971b-4d17bacb235f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {'grant access': 0,\n",
    " 'revoke access': 0,\n",
    " 'access profile': 0,\n",
    " 'disk space': 1,\n",
    " 'disk full': 1,\n",
    " 'disk error': 1,\n",
    " 'add user': 2,\n",
    " 'delete user': 2,\n",
    " 'create user': 2,\n",
    " 'modify user': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f04cee-2112-4ad9-9e26-db24c0151ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_set = dataset_to_df(support_tickets['train'], \n",
    "                       label=label_name_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c459f2-8684-44e8-9c09-653ef8654342",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_set.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca019e8-7653-4a8a-854e-188a0cc5f400",
   "metadata": {},
   "source": [
    "df_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85828428-668e-4a58-a092-86db1cb73ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index = np.random.permutation(df_set.index)\n",
    "random_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7368c38-3b24-4897-93da-e0ff47da8b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsets = ['train','valid','test']\n",
    "train_ratio = 0.6\n",
    "lng = len(random_index)\n",
    "train_start = 0\n",
    "train_end = int(lng*train_ratio)\n",
    "\n",
    "val_ratio = 0.2\n",
    "val_start = train_end+1\n",
    "val_end = int(lng*(train_ratio+val_ratio))\n",
    "\n",
    "test_start = val_end + 1 \n",
    "test_end = lng\n",
    "\n",
    "split_ind = np.array([[train_start,train_end],\n",
    "                      [val_start,val_end],\n",
    "                      [test_start,test_end]])\n",
    "\n",
    "ind2subset = {key:random_index[split_ind[i][0]:split_ind[i][1]] for i,key in enumerate(subsets)}\n",
    "ind2subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9af4e6b-9717-48db-a44f-c9607b4cec5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_set.iloc[ind2subset['valid']]['label'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6109c01d-879b-418f-a175-39a0d277af20",
   "metadata": {},
   "source": [
    "<b>1.3 Create a Dataset with Train, Validation and Test splits</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ddef4f-3947-48f4-89e8-7447379adf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_splits = DatasetDict({\n",
    "                        'train': Dataset.from_pandas(\n",
    "                                                    df_set.iloc[ind2subset['train']]\n",
    "                                                    ),\n",
    "                        'valid': Dataset.from_pandas(\n",
    "                                                    df_set.iloc[ind2subset['valid']]\n",
    "                                                    ),\n",
    "                        'test': Dataset.from_pandas(\n",
    "                                                    df_set.iloc[ind2subset['test']]\n",
    "                                                    )\n",
    "                        })\n",
    "ds_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de527a40-d6e0-406f-95aa-bcc41d7d8223",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsets = ['train','valid','test']\n",
    "split_dict = { key: Dataset.from_pandas(df_set.iloc[ind2subset[key]]) for key in subsets}\n",
    "support_tickets_splits = DatasetDict(split_dict)\n",
    "support_tickets_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf82a01",
   "metadata": {},
   "source": [
    "### 2. Train BERT base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4014827b-2f92-457b-83be-7709943a0746",
   "metadata": {},
   "source": [
    "<b>2.1 Load model from HuggingFace Hub</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa14cce7-dd90-439e-aa96-f743f10cee4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_name_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0311bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"google-bert/bert-base-uncased\"\n",
    "#model_path = 'distilbert-base-uncased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "label2id = label_name_dict.copy()\n",
    "id2label = {label2id[key]:key for key in label2id}\n",
    "#id2label = {0: \"access\", 1: \"disk\", 2: \"user\"}\n",
    "#label2id = {id2label[key]:key for key in id2label}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, \n",
    "                                                           num_labels=len(list(id2label.keys())), \n",
    "                                                           id2label=id2label, \n",
    "                                                           label2id=label2id\n",
    "                                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c1b471-fadb-44d5-8e42-f827749a6478",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66136d81-b7f6-4125-849e-4e0199afda7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826084b6",
   "metadata": {},
   "source": [
    "<b>2.2 Viz Model Stru</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03aa44f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print layers\n",
    "for name, param in model.named_parameters():\n",
    "   print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2e3400-8a57-4ff7-a1ae-cc9e212e1d92",
   "metadata": {},
   "source": [
    "<b>2.3 Freeze base model</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b569296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze base model parameters and unfreeze base model pooling layers\n",
    "for name, param in model.base_model.named_parameters():\n",
    "    if \"pooler\" in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161d7c59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print layers\n",
    "for name, param in model.named_parameters():\n",
    "   print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2e421a",
   "metadata": {},
   "source": [
    "#### 3. Training Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66acee3d-9f7c-4d4d-add9-b4e6d3b52f94",
   "metadata": {},
   "source": [
    "<b>3.1 Preprocess Text Data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0dc1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define text preprocessing\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"description\"], truncation='only_first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42616c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize all datasetse\n",
    "tokenized_data = support_tickets_splits.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a04a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2ce8c7",
   "metadata": {},
   "source": [
    "<b>3.2 Evaluation metrics</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b12c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load metrics\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "#auc_score = evaluate.load(\"roc_auc\")\n",
    "\n",
    "def compute_softmax(preds):\n",
    "    max_val = preds.max()\n",
    "    pred_exp = np.exp(preds-max_val)\n",
    "    probs = pred_exp/pred_exp.sum(-1, keepdims=True)\n",
    "    return probs \n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    # get predictions\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # apply softmax to get probabilities\n",
    "    probabilities = compute_softmax(predictions)\n",
    "    # use probabilities of the positive class for ROC AUC\n",
    "    positive_class_probs = probabilities[:, 1]\n",
    "    # compute auc\n",
    "    #auc = float(round(auc_score.compute(prediction_scores=positive_class_probs, references=labels)['roc_auc'],3))\n",
    "    \n",
    "    # predict most probable class\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    # compute accuracy\n",
    "    acc = float(round(accuracy.compute(predictions=predicted_classes, references=labels)['accuracy'],3))\n",
    "    \n",
    "    return {\"Accuracy\": acc}#, \"AUC\": auc}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68613386",
   "metadata": {},
   "source": [
    "#### 4. Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00032f77-a1d7-44b7-94ae-8cfb47970669",
   "metadata": {},
   "source": [
    "<b>4.1 Set Training Arguments</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579f1230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "lr = 1e-2\n",
    "batch_size = 8\n",
    "num_epochs = 100\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"bert_ticket_classifier\",\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    logging_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    use_cpu=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5512263-b4af-40ec-bfcc-5300de961fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5002ac-b3a6-43a0-b4af-eed77063577b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4482e9-ebaf-4bd4-8e36-31ad6820b593",
   "metadata": {},
   "source": [
    "<b>4.2 Define Trainer Module</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8e24f5-d794-46b0-8b42-66e3049741e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d169f1-d0f3-4303-8112-29115a3b0c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9a2714-b865-4075-8b0f-1b16f0b407b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.__dict__['args'].device = torch.device('cpu')\n",
    "trainer.args.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a105c6da-e0e9-42f0-912b-f623296c5148",
   "metadata": {},
   "source": [
    "<b>4.3 Fine-tune BERT Model</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cd1d59-ea44-4f6d-8825-d1c890714bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36865773-e6e2-4434-ad1d-4e8d42d2f7cd",
   "metadata": {},
   "source": [
    "<b>4.4 Extract Training History</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae41fcb7-9c9b-426b-b488-2f626c3d5b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.state.log_history[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7711fa-4b9c-48b7-bb22-14202ea9fecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.state.log_history[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e02a72a-895c-4d1d-b05d-66c31b847e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.state.log_history[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98301720-7d0d-4674-9ca7-331cc986114f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trainer.state.log_history[:-1]),len(trainer.state.log_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8ac094-ecdb-4fd3-a904-59c3fb785290",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_history = trainer.state.log_history[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14870d6b-3f1e-4b1e-9d22-37e1177c5aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_dict = {\n",
    "'train_loss': [],\n",
    "'valid_loss': [],\n",
    "'valid_acc': []\n",
    "}\n",
    "\n",
    "for item in learning_history:\n",
    "    if 'loss' in item:\n",
    "        hist_dict['train_loss'].append(item['loss'])\n",
    "        continue\n",
    "        \n",
    "    if 'eval_loss' in item:\n",
    "        hist_dict['valid_loss'].append(item['eval_loss'])\n",
    "        hist_dict['valid_acc'].append(item['eval_Accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996060fa-06e1-4f2d-86c6-bd1772deb091",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in  hist_dict:\n",
    "    print(key,hist_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16f8a62-208d-4860-8e21-136b8965b62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "for key in  list(hist_dict.keys())[:-1]:\n",
    "    plt.plot(hist_dict[key])\n",
    "plt.legend(list(hist_dict.keys())[:-1])\n",
    "plt.title('Train vs. Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "for key in  [list(hist_dict.keys())[-1]]:\n",
    "    plt.plot(hist_dict[key])\n",
    "plt.legend([list(hist_dict.keys())[-1]])\n",
    "plt.title('Validation Acc')\n",
    "plt.xlabel('Epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6b12cc",
   "metadata": {},
   "source": [
    "<b> 4.5 Apply Model to Validation Dataset</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1f2710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply model to validation dataset\n",
    "split_key = 'test'\n",
    "\n",
    "predictions = trainer.predict(tokenized_data[split_key])\n",
    "\n",
    "# Extract the logits and labels from the predictions object\n",
    "logits = predictions.predictions\n",
    "labels = predictions.label_ids\n",
    "\n",
    "# Use your compute_metrics function\n",
    "#metrics = compute_metrics((logits, labels))\n",
    "#print(metrics)\n",
    "true_count = 0\n",
    "for pred_l,actu_l in zip(labels,support_tickets_splits[split_key]['label']): \n",
    "    if pred_l == actu_l:\n",
    "        true_count += 1\n",
    "print(f'Accuracy {true_count/len(labels)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d02c7ab-744b-4848-8111-b7e916ba64c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0465f670-327f-48c4-b64b-0d50bd650684",
   "metadata": {},
   "outputs": [],
   "source": [
    "support_tickets_splits['test']['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff36f052-6ad2-48bf-8b1a-36597eaab1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "font_size = 12\n",
    "label_class = list(label2id.keys())\n",
    "cl_rep = classification_report(support_tickets_splits['test']['label'],\n",
    "                               labels,\n",
    "                               target_names=label_class,\n",
    "                               output_dict=True\n",
    "                            )\n",
    "\n",
    "cm = confusion_matrix(support_tickets_splits['test']['label'],\n",
    "                      labels)\n",
    "\n",
    "plt.figure(figsize = (8, 5))\n",
    "sns.heatmap(cm,\n",
    "            xticklabels=label_class,\n",
    "            yticklabels=label_class,\n",
    "            annot = True,  fmt = '.0f',\n",
    "            annot_kws={'size': font_size})\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1832793-145e-4784-8316-ece75174b197",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(cl_rep)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb89b178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # push model to hub\n",
    "# trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d56ddf5-7bf5-4261-a9bc-30c80ae17b12",
   "metadata": {},
   "source": [
    "## LORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e1c1c4-2f99-4a18-9818-cf1df96fd4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_path = \"google-bert/bert-base-uncased\"\n",
    "model_path_dict = {}\n",
    "model_path_dict['lora'] = 'distilbert-base-uncased'\n",
    "\n",
    "label2id = label_name_dict.copy()\n",
    "id2label = {label2id[key]:key for key in label2id}\n",
    "#id2label = {0: \"access\", 1: \"disk\", 2: \"user\"}\n",
    "#label2id = {id2label[key]:key for key in id2label}\n",
    "\n",
    "model_dict = {}\n",
    "\n",
    "model_dict['lora'] = AutoModelForSequenceClassification.from_pretrained(model_path_dict['lora'], \n",
    "                                                                        num_labels=len(list(id2label.keys())), \n",
    "                                                                        id2label=id2label, \n",
    "                                                                        label2id=label2id\n",
    "                                                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79f48bd-920f-40d2-a281-68679111d478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tokenizer\n",
    "tokenizer_dict = {}\n",
    "tokenizer_dict['lora'] = AutoTokenizer.from_pretrained(model_path_dict['lora'], \n",
    "                                                       add_prefix_space=True)\n",
    "\n",
    "# add pad token if none exists\n",
    "if tokenizer_dict['lora'].pad_token is None:\n",
    "    tokenizer_dict['lora'].add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer_dict['lora']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ddb32d-a8ca-43d8-a9fc-a6a3a3d1c4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_dict['lora']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5755d2-5af6-4c4f-9703-17827ffc84a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tokenize function\n",
    "def tokenize_function_lora(examples):\n",
    "    # extract text\n",
    "    text = examples[\"description\"]\n",
    "\n",
    "    #tokenize and truncate text\n",
    "    tokenizer_dict['lora'].truncation_side = \"left\"\n",
    "    tokenized_inputs = tokenizer_dict['lora'](text,\n",
    "                                              return_tensors=\"np\",\n",
    "                                              truncation=True,\n",
    "                                              max_length=512\n",
    "                                             )\n",
    "\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35a3ffe-d349-40d6-b556-a9956ec6a359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize training and validation datasets\n",
    "\n",
    "tokenized_dataset_dict = {}\n",
    "tokenized_dataset_dict['lora'] = ds_splits.map(tokenize_function_lora, \n",
    "                                               batched=True)\n",
    "tokenized_dataset_dict['lora']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6b9c7e-5bf1-456b-9cba-5aa16084a6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data collator\n",
    "data_collator_dict = {}\n",
    "\n",
    "data_collator_dict['lora'] = DataCollatorWithPadding(tokenizer=tokenizer_dict['lora'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608cf78c-0eab-4035-8c8b-c457229b093b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import accuracy evaluation metric\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "# define an evaluation function to pass into trainer later\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    return {\"accuracy\": accuracy.compute(predictions=predictions, references=labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edd7502-c29a-45ab-957b-56ce0a37821c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c13566-8393-48b1-b5a4-8e1627b1dee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model_lora.named_parameters():\n",
    "   print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b066abc4-2fbb-4c87-a180-be09e90d1fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(task_type=\"SEQ_CLS\",\n",
    "                        r=4,\n",
    "                        lora_alpha=32,\n",
    "                        lora_dropout=0.01,\n",
    "                        target_modules = ['q_lin','k_lin','v_lin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b42f7b6-89a9-4a3f-a57d-8bce62cd0e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict['lora'] = get_peft_model(model_dict['lora'], \n",
    "                                    peft_config)\n",
    "model_dict['lora'].print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e39adc1-1f8f-4867-b29b-c1673d3fbc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "lr = 1e-3\n",
    "batch_size = 16\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9471b699-13b7-4d05-bd00-d186fb238f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_arg_dict = {}\n",
    "# define training arguments\n",
    "train_arg_dict['lora'] = TrainingArguments(\n",
    "                                            output_dir= model_path_dict['lora'] + \"-lora-text-classification\",\n",
    "                                            learning_rate=lr,\n",
    "                                            per_device_train_batch_size=batch_size,\n",
    "                                            per_device_eval_batch_size=batch_size,\n",
    "                                            num_train_epochs=num_epochs,\n",
    "                                            weight_decay=0.01,\n",
    "                                            eval_strategy=\"epoch\",\n",
    "                                            save_strategy=\"epoch\",\n",
    "                                            load_best_model_at_end=True,\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3bee80-e298-4880-b983-55c0392cbcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_dict = {}\n",
    "trainer_dict['lora'] = Trainer(model=model_dict['lora'],\n",
    "                               args=train_arg_dict['lora'],\n",
    "                               train_dataset=tokenized_dataset_dict['lora'][\"train\"],\n",
    "                               eval_dataset=tokenized_dataset_dict['lora'][\"valid\"],\n",
    "                               tokenizer=tokenizer_dict['lora'],\n",
    "                               data_collator=data_collator_dict['lora'],\n",
    "                               compute_metrics=compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6415e91e-2113-4b0f-ba20-74d978825231",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_dict['lora'].train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befa38fe-2c72-4ffc-87f5-404b86859bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_key = \"test\"\n",
    "\n",
    "# apply model to validation dataset\n",
    "predictions = trainer.predict(tokenized_data[split_key])\n",
    "\n",
    "# Extract the logits and labels from the predictions object\n",
    "logits = predictions.predictions\n",
    "labels = predictions.label_ids\n",
    "\n",
    "# Use your compute_metrics function\n",
    "#metrics = compute_metrics((logits, labels))\n",
    "#print(metrics)\n",
    "true_count = 0\n",
    "for pred_l,actu_l in zip(labels,support_tickets_splits[split_key]['label']): \n",
    "    if pred_l == actu_l:\n",
    "        true_count += 1\n",
    "print(f'Accuracy {true_count/len(labels)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa1ad5a-006c-4342-a624-bd9b125b9402",
   "metadata": {},
   "outputs": [],
   "source": [
    "font_size = 12\n",
    "label_class = list(label2id.keys())\n",
    "cl_rep = classification_report(support_tickets_splits['test']['label'],\n",
    "                               labels,\n",
    "                               target_names=label_class,\n",
    "                               output_dict=True\n",
    "                            )\n",
    "\n",
    "cm = confusion_matrix(support_tickets_splits['test']['label'],\n",
    "                      labels)\n",
    "\n",
    "plt.figure(figsize = (8, 5))\n",
    "sns.heatmap(cm,\n",
    "            xticklabels=label_class,\n",
    "            yticklabels=label_class,\n",
    "            annot = True,  fmt = '.0f',\n",
    "            annot_kws={'size': font_size})\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0fd90c-b69a-49a8-8c21-7b8a6aa1c636",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
